## ‚ö° HPC Deployment Guide - Maximum Speed Training

### Understanding the CPU vs GPU Split

**PyBullet (CPU-bound):**
- Physics simulation: CPU
- Camera rendering: CPU
- Collision detection: CPU

**SAC Training (GPU-accelerated):**
- Neural network forward pass: GPU
- Gradient computation: GPU
- Backpropagation: GPU

**Key Insight**: We need **BOTH** powerful CPUs and GPU!

---

## üöÄ Speed Optimization Strategies

### 1. Vectorized Environments (10-15x speedup)

**Concept**: Run multiple PyBullet instances in parallel on different CPU cores

**Implementation**:
```bash
# On HPC with 32 cores
python train_curriculum_fast.py \
    --n_envs 16 \
    --total_timesteps 1000000 \
    --device cuda
```

**Speedup Breakdown**:
- Single env: 4.5 steps/s
- 16 parallel envs: ~60-70 steps/s (13-15x)
- 1M steps: 62 hours ‚Üí **4-5 hours**

---

### 2. PyBullet Physics Optimization (2x speedup)

Apply these settings in your environment:

```python
# In CubeTrackingEnv.__init__():

# Reduce solver iterations
p.setPhysicsEngineParameter(numSolverIterations=20)  # was 50

# Larger timestep
p.setTimeStep(1.0 / 120.0)  # was 1/240

# Lower resolution camera
self.camera_width = 320   # was 480
self.camera_height = 240  # was 640
```

**Total speedup**: ~2x faster simulation

---

### 3. Larger Batch Sizes (GPU efficiency)

**On HPC with A100 GPU (40GB memory)**:
```python
model = SAC(
    batch_size=1024,      # vs 256 on laptop
    buffer_size=500000,   # vs 100k on laptop
    policy_kwargs=dict(net_arch=[512, 512])  # Larger network
)
```

**Benefit**: Better GPU utilization, faster gradient updates

---

## üìã Complete HPC SLURM Script

Create `slurm_train_curriculum.sh`:

```bash
#!/bin/bash
#SBATCH --job-name=curriculum_rl
#SBATCH --output=logs/curriculum_%j.out
#SBATCH --error=logs/curriculum_%j.err

# Resource allocation
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=32        # CRITICAL: Request many cores!
#SBATCH --gres=gpu:a100:1          # A100 GPU (or a6000, v100)
#SBATCH --mem=64G                  # 64GB RAM for multiple envs
#SBATCH --time=24:00:00            # 24 hours max

# Partition (adjust for your HPC)
#SBATCH --partition=gpu

# Email notifications (optional)
#SBATCH --mail-type=END,FAIL
#SBATCH --mail-user=your.email@university.edu

echo "========================================="
echo "Job started: $(date)"
echo "Job ID: $SLURM_JOB_ID"
echo "Node: $SLURM_NODELIST"
echo "CPUs: $SLURM_CPUS_PER_TASK"
echo "========================================="

# Load modules (adjust for your HPC)
module purge
module load python/3.10
module load cuda/12.4
module load gcc/11.2.0

# Activate virtual environment
source $HOME/rl_env/bin/activate

# Navigate to project directory
cd $HOME/Reinforcement_Learning/Pybullet

# Verify GPU
nvidia-smi

# Set number of threads for NumPy/PyTorch
export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK
export MKL_NUM_THREADS=$SLURM_CPUS_PER_TASK

# Run training with optimized settings
python train_curriculum_fast.py \
    --n_envs 16 \
    --total_timesteps 1000000 \
    --buffer_size 300000 \
    --batch_size 1024 \
    --checkpoint_freq 100000 \
    --log_dir curriculum_runs_hpc \
    --run_name "curriculum_hpc_${SLURM_JOB_ID}" \
    --device cuda

echo "========================================="
echo "Job completed: $(date)"
echo "========================================="
```

**Submit job**:
```bash
sbatch slurm_train_curriculum.sh
```

---

## üìä Expected Performance on HPC

### Configuration Comparison

| Setup | Cores | GPU | Envs | Steps/s | 1M Steps Time |
|-------|-------|-----|------|---------|---------------|
| **Laptop** | 8 | RTX 3050 | 1 | 4.5 | 62 hours |
| **HPC Basic** | 16 | V100 | 8 | 35 | 8 hours |
| **HPC Optimized** | 32 | A100 | 16 | 65 | 4.3 hours |
| **HPC Maximum** | 64 | A100 | 32 | 110 | 2.5 hours |

### Recommended Configuration

**For 1M step curriculum training**:
```bash
#SBATCH --cpus-per-task=32
#SBATCH --gres=gpu:a100:1
#SBATCH --mem=64G

python train_curriculum_fast.py --n_envs 16
```

**Expected time**: 4-5 hours (vs 62 hours on laptop)

---

## üîß Step-by-Step HPC Deployment

### 1. Setup on HPC

```bash
# SSH to HPC
ssh username@hpc.university.edu

# Create virtual environment
module load python/3.10
python -m venv ~/rl_env
source ~/rl_env/bin/activate

# Install dependencies
pip install --upgrade pip
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121
pip install stable-baselines3
pip install pybullet
pip install tensorboard
pip install matplotlib
pip install gym==0.21.0

# Test installation
python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}')"
python -c "import pybullet; print('PyBullet OK')"
```

### 2. Transfer Code to HPC

```bash
# From your laptop
rsync -avz --exclude='*.pyc' --exclude='__pycache__' \
    ~/Reinforcement_Learning/Pybullet/ \
    username@hpc.university.edu:~/Reinforcement_Learning/Pybullet/

# Or use git (recommended)
git init
git add .
git commit -m "Initial commit"
git push origin main

# On HPC
git clone your-repo-url
```

### 3. Create Log Directory

```bash
mkdir -p ~/Reinforcement_Learning/Pybullet/logs
mkdir -p ~/Reinforcement_Learning/Pybullet/curriculum_runs_hpc
```

### 4. Submit Training Job

```bash
cd ~/Reinforcement_Learning/Pybullet
sbatch slurm_train_curriculum.sh

# Check job status
squeue -u $USER

# Monitor output
tail -f logs/curriculum_<JOBID>.out
```

---

## üì° Monitoring Training on HPC

### Check Job Status

```bash
# List your jobs
squeue -u $USER

# Job details
scontrol show job <JOBID>

# Cancel job
scancel <JOBID>
```

### Monitor TensorBoard Remotely

**Option 1: SSH Tunnel**
```bash
# On HPC (once job is running)
tensorboard --logdir curriculum_runs_hpc --port 6006 --host 0.0.0.0

# On your laptop (in another terminal)
ssh -L 6006:compute-node:6006 username@hpc.university.edu

# Open browser
http://localhost:6006
```

**Option 2: Download logs periodically**
```bash
# On your laptop
rsync -avz username@hpc.university.edu:~/Reinforcement_Learning/Pybullet/curriculum_runs_hpc/ \
    ./hpc_results/

# View locally
tensorboard --logdir hpc_results/
```

---

## ‚öôÔ∏è Optimization Settings Summary

### For Maximum Speed (Recommended)

**Environment Setup** (`train_curriculum_fast.py`):
- ‚úÖ 16 parallel environments (SubprocVecEnv)
- ‚úÖ 32 CPU cores allocated

**PyBullet Settings**:
- ‚úÖ Solver iterations: 20 (vs 50)
- ‚úÖ Timestep: 1/120s (vs 1/240s)
- ‚úÖ Camera: 320x240 (vs 480x640)

**SAC Hyperparameters**:
- ‚úÖ Batch size: 1024 (vs 256)
- ‚úÖ Buffer size: 300k (vs 100k)
- ‚úÖ Network: [512, 512] (vs [256, 256])

**Expected Combined Speedup**: **20-25x faster than laptop!**

---

## üéØ Training Time Estimates (HPC with Optimizations)

| Total Steps | Laptop Time | HPC Time (16 envs) | HPC Time (32 envs) |
|-------------|-------------|--------------------|--------------------|
| 100k | 6 hours | 20 min | 10 min |
| 500k | 31 hours | 1.7 hours | 50 min |
| 1M | 62 hours | **3.4 hours** | **1.7 hours** |
| 2M | 124 hours (5 days) | **6.8 hours** | **3.4 hours** |

**For your thesis (1M steps)**:
- Curriculum: 3.4 hours
- Baseline: 3.4 hours
- **Total**: ~7 hours of HPC time

**You can run full experiments in a single day!**

---

## üö® Common HPC Issues & Solutions

### Issue 1: "Too many open files"

**Solution**: Reduce number of parallel environments
```bash
python train_curriculum_fast.py --n_envs 8  # instead of 16
```

### Issue 2: GPU Out of Memory

**Solution**: Reduce batch size or buffer
```bash
python train_curriculum_fast.py --batch_size 512 --buffer_size 200000
```

### Issue 3: PyBullet GUI errors

**Solution**: Already using p.DIRECT mode ‚úì

### Issue 4: Slow file I/O

**Solution**: Use local scratch space
```bash
#SBATCH --tmp=20G  # Request local SSD

# In script:
export TMPDIR=/scratch/$SLURM_JOB_ID
cd $TMPDIR
# Copy data, run training, copy results back
```

---

## üìã Complete HPC Workflow

### Day 1: Setup and Test
```bash
# 1. Setup environment
ssh hpc
module load python cuda
pip install -r requirements.txt

# 2. Quick test (1k steps)
python train_curriculum_fast.py --total_timesteps 1000 --n_envs 4

# 3. Verify it works
```

### Day 2: Full Training
```bash
# 1. Submit curriculum job
sbatch slurm_train_curriculum.sh

# 2. Submit baseline job (for comparison)
sbatch slurm_train_baseline.sh

# 3. Monitor progress
tail -f logs/*.out

# 4. Check TensorBoard
ssh -L 6006:node:6006 hpc
tensorboard --logdir curriculum_runs_hpc
```

### Day 3: Analysis
```bash
# 1. Download results
rsync -avz hpc:~/Reinforcement_Learning/Pybullet/curriculum_runs_hpc/ ./results/

# 2. Generate plots
python visualize_training.py results/curriculum_* --plot

# 3. Write thesis Section 3.4 with results
```

---

## üéØ Recommended Settings for Your Thesis

**Quick Results (500k steps, ~2 hours)**:
```bash
python train_curriculum_fast.py \
    --n_envs 16 \
    --total_timesteps 500000 \
    --batch_size 1024 \
    --buffer_size 200000
```

**Full Results (1M steps, ~3.5 hours)**:
```bash
python train_curriculum_fast.py \
    --n_envs 16 \
    --total_timesteps 1000000 \
    --batch_size 1024 \
    --buffer_size 300000 \
    --checkpoint_freq 100000
```

**Publication Quality (2M steps, multiple seeds, ~24 hours)**:
```bash
for seed in 42 123 456; do
    sbatch --export=SEED=$seed slurm_train_curriculum.sh
done
```

---

## üìä Summary: Speedup Breakdown

| Optimization | Speedup | Cumulative |
|--------------|---------|------------|
| Baseline (laptop) | 1x | 1x |
| + Better GPU (A100) | 1.5x | 1.5x |
| + 16 parallel envs | 12x | **18x** |
| + PyBullet optimization | 2x | **36x** |
| + Larger batches | 1.3x | **47x** |

**Final Result**: 1M steps in **~1.3 hours** (vs 62 hours on laptop)

**You can run your ENTIRE thesis experiments in one day on HPC!**

---

**Next Steps**:
1. Test `train_curriculum_fast.py` locally with `--n_envs 4`
2. Create SLURM script for your specific HPC system
3. Submit test job (10k steps) to verify setup
4. Run full training (500k-1M steps)
5. Analyze results and write thesis!
